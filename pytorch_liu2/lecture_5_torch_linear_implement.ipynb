{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 8.245771408081055\n",
      "1 3.8770594596862793\n",
      "2 1.9292683601379395\n",
      "3 1.0592445135116577\n",
      "4 0.6690552830696106\n",
      "5 0.49251532554626465\n",
      "6 0.41112715005874634\n",
      "7 0.3721380829811096\n",
      "8 0.35206329822540283\n",
      "9 0.3404475748538971\n",
      "10 0.3326365351676941\n",
      "11 0.326556533575058\n",
      "12 0.3212851285934448\n",
      "13 0.3164101839065552\n",
      "14 0.31174829602241516\n",
      "15 0.30721673369407654\n",
      "16 0.3027791380882263\n",
      "17 0.2984173595905304\n",
      "18 0.29412412643432617\n",
      "19 0.2898953855037689\n",
      "20 0.28572797775268555\n",
      "21 0.2816213369369507\n",
      "22 0.27757394313812256\n",
      "23 0.2735844552516937\n",
      "24 0.26965275406837463\n",
      "25 0.2657774090766907\n",
      "26 0.26195767521858215\n",
      "27 0.25819286704063416\n",
      "28 0.25448235869407654\n",
      "29 0.25082483887672424\n",
      "30 0.24722038209438324\n",
      "31 0.2436670958995819\n",
      "32 0.24016529321670532\n",
      "33 0.23671367764472961\n",
      "34 0.23331180214881897\n",
      "35 0.2299586534500122\n",
      "36 0.2266540825366974\n",
      "37 0.2233966886997223\n",
      "38 0.22018611431121826\n",
      "39 0.21702167391777039\n",
      "40 0.21390271186828613\n",
      "41 0.21082836389541626\n",
      "42 0.2077985256910324\n",
      "43 0.2048121988773346\n",
      "44 0.20186878740787506\n",
      "45 0.19896776974201202\n",
      "46 0.19610796868801117\n",
      "47 0.19328971207141876\n",
      "48 0.19051189720630646\n",
      "49 0.18777382373809814\n",
      "50 0.18507502973079681\n",
      "51 0.18241554498672485\n",
      "52 0.17979399859905243\n",
      "53 0.1772097796201706\n",
      "54 0.1746632605791092\n",
      "55 0.17215301096439362\n",
      "56 0.1696789264678955\n",
      "57 0.16724027693271637\n",
      "58 0.16483686864376068\n",
      "59 0.1624678373336792\n",
      "60 0.1601330041885376\n",
      "61 0.1578313708305359\n",
      "62 0.1555631011724472\n",
      "63 0.15332740545272827\n",
      "64 0.15112419426441193\n",
      "65 0.14895205199718475\n",
      "66 0.1468113362789154\n",
      "67 0.1447014957666397\n",
      "68 0.14262200891971588\n",
      "69 0.14057226479053497\n",
      "70 0.13855205476284027\n",
      "71 0.13656070828437805\n",
      "72 0.13459818065166473\n",
      "73 0.13266395032405853\n",
      "74 0.13075730204582214\n",
      "75 0.1288779228925705\n",
      "76 0.12702599167823792\n",
      "77 0.1252003014087677\n",
      "78 0.12340093404054642\n",
      "79 0.12162767350673676\n",
      "80 0.11987943202257156\n",
      "81 0.11815665662288666\n",
      "82 0.1164586991071701\n",
      "83 0.11478491872549057\n",
      "84 0.11313522607088089\n",
      "85 0.11150925606489182\n",
      "86 0.10990658402442932\n",
      "87 0.10832706838846207\n",
      "88 0.1067703515291214\n",
      "89 0.10523587465286255\n",
      "90 0.1037234291434288\n",
      "91 0.10223280638456345\n",
      "92 0.10076355934143066\n",
      "93 0.09931548684835434\n",
      "94 0.09788811206817627\n",
      "95 0.09648145735263824\n",
      "96 0.09509473294019699\n",
      "97 0.09372805804014206\n",
      "98 0.09238114207983017\n",
      "99 0.09105344861745834\n",
      "w =  1.7991183996200562\n",
      "b =  0.4566510021686554\n",
      "y_pred =  tensor([[7.6531]])\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "# prepare dataset\n",
    "# x,y是矩阵，3行1列 也就是说总共有3个数据，每个数据只有1个特征\n",
    "x_data = torch.tensor([[1.0], [2.0], [3.0]])\n",
    "y_data = torch.tensor([[2.0], [4.0], [6.0]])\n",
    " \n",
    "#design model using class\n",
    "\"\"\"\n",
    "our model class should be inherit from nn.Module, which is base class for all neural network modules.\n",
    "member methods __init__() and forward() have to be implemented\n",
    "class nn.linear contain two member Tensors: weight and bias\n",
    "class nn.Linear has implemented the magic method __call__(),which enable the instance of the class can\n",
    "be called just like a function.Normally the forward() will be called \n",
    "\"\"\"\n",
    "class LinearModel(torch.nn.Module):\n",
    "    def __init__(self):\n",
    "        super(LinearModel, self).__init__()\n",
    "        # (1,1)是指输入x和输出y的特征维度，这里数据集中的x和y的特征都是1维的\n",
    "        # 该线性层需要学习的参数是w和b  获取w/b的方式分别是~linear.weight/linear.bias\n",
    "        self.linear = torch.nn.Linear(1, 1)\n",
    " \n",
    "    def forward(self, x):\n",
    "        y_pred = self.linear(x)\n",
    "        return y_pred\n",
    " \n",
    "model = LinearModel()\n",
    " \n",
    "# construct loss and optimizer\n",
    "# criterion = torch.nn.MSELoss(size_average = False)\n",
    "criterion = torch.nn.MSELoss(reduction = 'sum')\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr = 0.01) # model.parameters()自动完成参数的初始化操作，这个地方我可能理解错了\n",
    " \n",
    "# training cycle forward, backward, update\n",
    "for epoch in range(100):\n",
    "    y_pred = model(x_data) # forward:predict\n",
    "    loss = criterion(y_pred, y_data) # forward: loss\n",
    "    print(epoch, loss.item())\n",
    " \n",
    "    optimizer.zero_grad() # the grad computer by .backward() will be accumulated. so before backward, remember set the grad to zero\n",
    "    loss.backward() # backward: autograd，自动计算梯度\n",
    "    optimizer.step() # update 参数，即更新w和b的值\n",
    " \n",
    "print('w = ', model.linear.weight.item())\n",
    "print('b = ', model.linear.bias.item())\n",
    " \n",
    "x_test = torch.tensor([[4.0]])\n",
    "y_test = model(x_test)\n",
    "print('y_pred = ', y_test.data)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "对象初始化。。。。\n",
      "我年龄是: 10\n",
      "forward 函数被调用了\n",
      "我现在的年龄是： 12\n"
     ]
    }
   ],
   "source": [
    "class A():\n",
    "    def __init__(self, init_age):\n",
    "        super().__init__()\n",
    "        print('我年龄是:',init_age)\n",
    "        self.age = init_age\n",
    " \n",
    "    def __call__(self, added_age):\n",
    "        \n",
    " \n",
    "        res = self.forward(added_age)\n",
    "        return res\n",
    " \n",
    "    def forward(self, input_):\n",
    "        print('forward 函数被调用了')\n",
    "        \n",
    "        return input_ + self.age\n",
    "print('对象初始化。。。。')\n",
    "a = A(10) #所以这一行就是在创建对象的时候就给的self参数\n",
    " \n",
    " \n",
    "input_param = a(2)#这行是实例化过后对call函数的调用\n",
    "\n",
    "print(\"我现在的年龄是：\", input_param)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([128, 30])"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "\n",
    "m = nn.Linear(20, 30)\n",
    "input = torch.randn(128, 20)\n",
    "output = m(input)\n",
    "\n",
    "output.size()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "刚开始看这份代码是有点迷惑的，m是类对象，而直接像函数一样调用m，m(input)\n",
    "\n",
    "重点：\n",
    "nn.Module 是所有神经网络单元（neural network modules）的基类\n",
    "pytorch在nn.Module中，实现了__call__方法，而在__call__方法中调用了forward函数。\n",
    "经过以上两点。上述代码就不难理解。\n",
    "\n",
    "而nn.Linear的forward返回的是input*weight+bias\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 所以自己创建多层神经网络模块时，只需要在实现__init__和forward即可.\n",
    "# 接下来看一个简单的三层神经网络的例子：\n",
    "\n",
    "# define three layers\n",
    "class simpleNet(nn.Module):\n",
    "\n",
    "    def __init__(self, in_dim, n_hidden_1, n_hidden_2, out_dim):\n",
    "        super().__init__()\n",
    "        self.layer1 = nn.Linear(in_dim, n_hidden_1)\n",
    "        self.layer2 = nn.Linear(n_hidden_1, n_hidden_2)\n",
    "        self.layer3 = nn.Linear(n_hidden_2, out_dim)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.layer1(x)\n",
    "        x = self.layer2(x)\n",
    "        x = self.layer3(x)\n",
    "\n",
    "        return x\n",
    "# 以下为各层神经元个数：\n",
    "# 输入： in_dim\n",
    "# 第一层： n_hidden_1\n",
    "# 第二层：n_hidden_2\n",
    "# 第三层（输出层）：out_dim"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pytorch",
   "language": "python",
   "name": "pytorch"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
